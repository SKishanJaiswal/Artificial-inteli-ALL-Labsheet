import random

class Perceptron:
    def __init__(self, lr=0.1, epochs=100):
        self.lr = lr
        self.epochs = epochs
        self.weights = [random.uniform(-1, 1), random.uniform(-1, 1)]
        self.bias = random.uniform(-1, 1)

    def step_activation(self, x):
        return 1 if x >= 0 else 0

    def train(self, inputs, targets):
        print(f"Initial weights: {self.weights}, Bias: {self.bias:.3f}")
        print("Epoch Input1 Input2 Weight1 Weight2 Output Error ChangeW1 ChangeW2 NewW1   NewW2")
        for epoch in range(self.epochs):
            errors = 0
            for inp, target in zip(inputs, targets):
                w1_before, w2_before = self.weights  # store current weights
                summation = sum(w * i for w, i in zip(self.weights, inp)) + self.bias
                output = self.step_activation(summation)
                error = target - output

                # calculate changes
                change_w1 = self.lr * error * inp[0]
                change_w2 = self.lr * error * inp[1]

                # update weights
                new_w1 = w1_before + change_w1
                new_w2 = w2_before + change_w2
                self.weights = [new_w1, new_w2]
                self.bias += self.lr * error

                if error != 0:
                    errors += 1

                print(f"{epoch+1:5} {inp[0]:6} {inp[1]:6} {w1_before:7.3f} {w2_before:7.3f} "
                      f"{output:6} {error:5} {change_w1:8.3f} {change_w2:8.3f} "
                      f"{new_w1:7.3f} {new_w2:7.3f}")
            if errors == 0:
                break

    def evaluate(self, inputs, targets):
        correct = 0
        for inp, target in zip(inputs, targets):
            summation = sum(w * i for w, i in zip(self.weights, inp)) + self.bias
            prediction = self.step_activation(summation)
            if prediction == target:
                correct += 1
        return correct / len(inputs)

if __name__ == "__main__":
    inputs = [[0,0], [0,1], [1,0], [1,1]]

    print("Training Perceptron for AND gate:")
    targets_and = [0, 0, 0, 1]
    perceptron_and = Perceptron(lr=0.1, epochs=100)
    perceptron_and.train(inputs, targets_and)
    acc_and = perceptron_and.evaluate(inputs, targets_and)
    print(f"Final accuracy for AND gate: {acc_and:.2f}")

    print("\nTraining Perceptron for OR gate:")
    targets_or = [0, 1, 1, 1]
    perceptron_or = Perceptron(lr=0.1, epochs=100)
    perceptron_or.train(inputs, targets_or)
    acc_or = perceptron_or.evaluate(inputs, targets_or)
    print(f"Final accuracy for OR gate: {acc_or:.2f}")

#code 2
import random
import itertools

class Perceptron:
    def __init__(self, input_size, lr=0.1, epochs=100):
        self.lr = lr
        self.epochs = epochs
        self.weights = [random.uniform(-1, 1) for _ in range(input_size)]
        self.bias = random.uniform(-1, 1)

    def step_activation(self, x):
        return 1 if x >= 0 else 0

    def train(self, inputs, targets):
        for epoch in range(self.epochs):
            errors = 0
            for inp, target in zip(inputs, targets):
                summation = sum(w * i for w, i in zip(self.weights, inp)) + self.bias
                output = self.step_activation(summation)
                error = target - output
                if error != 0:
                    errors += 1
                    # Update weights and bias
                    self.weights = [
                        w + self.lr * error * i for w, i in zip(self.weights, inp)
                    ]
                    self.bias += self.lr * error
            if errors == 0:
                break  # stop if perfectly classified

    def evaluate(self, inputs, targets):
        correct = 0
        for inp, target in zip(inputs, targets):
            summation = sum(w * i for w, i in zip(self.weights, inp)) + self.bias
            prediction = self.step_activation(summation)
            if prediction == target:
                correct += 1
        return correct / len(inputs)

def generate_truth_table(n):
    """Generates truth table with n inputs"""
    return list(itertools.product([0, 1], repeat=n))

def generate_targets(inputs, gate_type):
    """Creates target outputs for AND or OR gates"""
    if gate_type == 'AND':
        return [int(all(inp)) for inp in inputs]
    elif gate_type == 'OR':
        return [int(any(inp)) for inp in inputs]
    else:
        raise ValueError("Invalid gate_type: choose 'AND' or 'OR'")

if __name__ == "__main__":
    for n in [3, 4]:
        print(f"\n--- Training Perceptron for {n}-input AND gate ---")
        inputs = generate_truth_table(n)
        targets_and = generate_targets(inputs, 'AND')
        perceptron_and = Perceptron(input_size=n, lr=0.1, epochs=100)
        perceptron_and.train(inputs, targets_and)
        accuracy_and = perceptron_and.evaluate(inputs, targets_and)
        print(f"Final weights: {perceptron_and.weights}")
        print(f"Final bias: {perceptron_and.bias:.3f}")
        print(f"Accuracy: {accuracy_and:.2f}")

        print(f"\n--- Training Perceptron for {n}-input OR gate ---")
        targets_or = generate_targets(inputs, 'OR')
        perceptron_or = Perceptron(input_size=n, lr=0.1, epochs=100)
        perceptron_or.train(inputs, targets_or)
        accuracy_or = perceptron_or.evaluate(inputs, targets_or)
        print(f"Final weights: {perceptron_or.weights}")
        print(f"Final bias: {perceptron_or.bias:.3f}")
        print(f"Accuracy: {accuracy_or:.2f}")

code 3
import random

class LinearPerceptron:
    def __init__(self, input_size, lr=0.01, epochs=100):
        self.lr = lr
        self.epochs = epochs
        self.weights = [random.uniform(-1, 1) for _ in range(input_size)]
        self.bias = random.uniform(-1, 1)

    def train(self, inputs, targets):
        for epoch in range(self.epochs):
            total_error = 0
            for inp, target in zip(inputs, targets):
                # Linear activation: output = weighted sum + bias
                output = sum(w * i for w, i in zip(self.weights, inp)) + self.bias
                error = target - output
                total_error += error ** 2
                # Update weights and bias
                self.weights = [w + self.lr * error * i for w, i in zip(self.weights, inp)]
                self.bias += self.lr * error
            mse = total_error / len(inputs)
            print(f"Epoch {epoch+1:3}: MSE = {mse:.4f}")
        print(f"\nFinal learned weights: {self.weights}")
        print(f"Final learned bias: {self.bias:.3f}")

if __name__ == "__main__":
    random.seed(42)  # For reproducibility

    # Generate 10 random samples: x1, x2, x3 in [0,1]
    inputs = [[random.random(), random.random(), random.random()] for _ in range(10)]

    # Compute targets: y = 2x1 + 3x2 - x3 + 5
    targets = [2*x[0] + 3*x[1] - x[2] + 5 for x in inputs]

    perceptron = LinearPerceptron(input_size=3, lr=0.01, epochs=100)
    perceptron.train(inputs, targets)


#code 4
import random

class LinearPerceptron:
    def __init__(self, input_size, lr=0.01, epochs=100):
        self.lr = lr
        self.epochs = epochs
        # Initialize weights and bias randomly
        self.weights = [random.uniform(-1, 1) for _ in range(input_size)]
        self.bias = random.uniform(-1, 1)

    def train(self, inputs, targets):
        for epoch in range(1, self.epochs+1):
            total_error = 0
            for inp, target in zip(inputs, targets):
                output = sum(w * i for w, i in zip(self.weights, inp)) + self.bias
                error = target - output
                total_error += error ** 2
                # Update weights and bias
                self.weights = [w + self.lr * error * i for w, i in zip(self.weights, inp)]
                self.bias += self.lr * error
            mse = total_error / len(inputs)
            print(f"Epoch {epoch:3}: MSE = {mse:.4f}")
        print(f"Final learned weights: {[round(w, 4) for w in self.weights]}")
        print(f"Final learned bias: {round(self.bias, 4)}")

def generate_dataset(n, num_samples=10):
    # Random true weights and bias=5
    true_weights = [random.uniform(-1, 1) for _ in range(n)]
    bias = 5
    # Generate random inputs and compute targets
    inputs = [[random.random() for _ in range(n)] for _ in range(num_samples)]
    targets = [sum(w * x for w, x in zip(true_weights, sample)) + bias for sample in inputs]
    print(f"True weights: {[round(w, 4) for w in true_weights]}, True bias: {bias}")
    return inputs, targets

if __name__ == "__main__":
    random.seed(42)  # for reproducibility

    for n in [4, 5]:
        print(f"\n=== Training Linear Perceptron with n={n} features ===")
        inputs, targets = generate_dataset(n)
        perceptron = LinearPerceptron(input_size=n, lr=0.01, epochs=100)
        perceptron.train(inputs, targets)
